{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EPA_with_Covid_19.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FObs8FvVIpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/drive/My Drive/Deep Learning Data/Covid-19 Text Mining/CORD-19-research-challenge.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7edB11YzVTHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import glob\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2BWP_miW8he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path = '/content/'\n",
        "metadata_path = f'{root_path}/metadata.csv'\n",
        "meta_df = pd.read_csv(metadata_path, dtype={\n",
        "    'pubmed_id': str,\n",
        "    'Microsoft Academic Paper ID': str, \n",
        "    'doi': str\n",
        "})\n",
        "meta_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lJWL3OUXCJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_df.info()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWFGGh5LXIZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Getch all of JSON file Path\n",
        "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
        "len(all_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXIBUHndXR5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Helper function to Read articles\n",
        "class FileReader:\n",
        "    def __init__(self, file_path):\n",
        "        with open(file_path) as file:\n",
        "            content = json.load(file)\n",
        "            self.paper_id = content['paper_id']\n",
        "            self.abstract = []\n",
        "            self.body_text = []\n",
        "            # Abstract\n",
        "            for entry in content['abstract']:\n",
        "                self.abstract.append(entry['text'])\n",
        "            # Body text\n",
        "            for entry in content['body_text']:\n",
        "                self.body_text.append(entry['text'])\n",
        "            self.abstract = '\\n'.join(self.abstract)\n",
        "            self.body_text = '\\n'.join(self.body_text)\n",
        "    def __repr__(self):\n",
        "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n",
        "first_row = FileReader(all_json[0])\n",
        "print(first_row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy9DE4hHXxXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## helper function to break after everywords when character length reach to certain amount\n",
        "def get_breaks(content, length):\n",
        "    data = \"\"\n",
        "    words = content.split(' ')\n",
        "    total_chars = 0\n",
        "\n",
        "    # add break every length characters\n",
        "    for i in range(len(words)):\n",
        "        total_chars += len(words[i])\n",
        "        if total_chars > length:\n",
        "            data = data + \"<br>\" + words[i]\n",
        "            total_chars = 0\n",
        "        else:\n",
        "            data = data + \" \" + words[i]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAi1EoxIX4Bz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
        "for idx, entry in enumerate(all_json):\n",
        "    if idx % (len(all_json) // 10) == 0:\n",
        "        print(f'Processing index: {idx} of {len(all_json)}')\n",
        "    \n",
        "    try:\n",
        "        content = FileReader(entry)\n",
        "    except Exception as e:\n",
        "        continue  # invalid paper format, skip\n",
        "    \n",
        "    # get metadata information\n",
        "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
        "    # no metadata, skip this paper\n",
        "    if len(meta_data) == 0:\n",
        "        continue\n",
        "    \n",
        "    dict_['abstract'].append(content.abstract)\n",
        "    dict_['paper_id'].append(content.paper_id)\n",
        "    dict_['body_text'].append(content.body_text)\n",
        "    \n",
        "    # also create a column for the summary of abstract to be used in a plot\n",
        "    if len(content.abstract) == 0: \n",
        "        # no abstract provided\n",
        "        dict_['abstract_summary'].append(\"Not provided.\")\n",
        "    elif len(content.abstract.split(' ')) > 100:\n",
        "        # abstract provided is too long for plot, take first 100 words append with ...\n",
        "        info = content.abstract.split(' ')[:100]\n",
        "        summary = get_breaks(' '.join(info), 40)\n",
        "        dict_['abstract_summary'].append(summary + \"...\")\n",
        "    else:\n",
        "        # abstract is short enough\n",
        "        summary = get_breaks(content.abstract, 40)\n",
        "        dict_['abstract_summary'].append(summary)\n",
        "        \n",
        "    # get metadata information\n",
        "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
        "    \n",
        "    try:\n",
        "        # if more than one author\n",
        "        authors = meta_data['authors'].values[0].split(';')\n",
        "        if len(authors) > 2:\n",
        "            # if more than 2 authors, take them all with html tag breaks in between\n",
        "            dict_['authors'].append(get_breaks('. '.join(authors), 40))\n",
        "        else:\n",
        "            # authors will fit in plot\n",
        "            dict_['authors'].append(\". \".join(authors))\n",
        "    except Exception as e:\n",
        "        # if only one author - or Null valie\n",
        "        dict_['authors'].append(meta_data['authors'].values[0])\n",
        "    \n",
        "    # add the title information, add breaks when needed\n",
        "    try:\n",
        "        title = get_breaks(meta_data['title'].values[0], 40)\n",
        "        dict_['title'].append(title)\n",
        "    # if title was not provided\n",
        "    except Exception as e:\n",
        "        dict_['title'].append(meta_data['title'].values[0])\n",
        "    \n",
        "    # add the journal information\n",
        "    dict_['journal'].append(meta_data['journal'].values[0])\n",
        "    \n",
        "    # add doi\n",
        "    dict_['doi'].append(meta_data['doi'].values[0])\n",
        "    \n",
        "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\n",
        "df_covid.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT8CWYUSYC7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Some feature engineerings\n",
        "df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\n",
        "df_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\n",
        "df_covid['body_unique_words']=df_covid['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\n",
        "df_covid.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHfITWJPZVyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_covid.info()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_GV-61IZnqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_covid['abstract'].describe(include='all')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVoU366NZpkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Drop duplicates\n",
        "df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\n",
        "df_covid['abstract'].describe(include='all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7U7BxwRZuak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_covid['body_text'].describe(include='all')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpEOC1HTZxdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_covid.describe()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtbzLay4Z3rK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Drop NAN\n",
        "df_covid.dropna(inplace=True)\n",
        "df_covid.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w19e1Fqaenh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QychHS3iaGuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Handling Multilanguage Texts. @ Credit to ....... Kaggle\n",
        "\n",
        "from tqdm import tqdm\n",
        "from langdetect import detect\n",
        "from langdetect import DetectorFactory\n",
        "\n",
        "# set seed\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# hold label - language\n",
        "languages = []\n",
        "\n",
        "# go through each text\n",
        "for ii in tqdm(range(0,len(df_covid))):\n",
        "    # split by space into list, take the first x intex, join with space\n",
        "    text = df_covid.iloc[ii]['body_text'].split(\" \")\n",
        "    \n",
        "    lang = \"en\"\n",
        "    try:\n",
        "        if len(text) > 50:\n",
        "            lang = detect(\" \".join(text[:50]))\n",
        "        elif len(text) > 0:\n",
        "            lang = detect(\" \".join(text[:len(text)]))\n",
        "    # ught... beginning of the document was not in a good format\n",
        "    except Exception as e:\n",
        "        all_words = set(text)\n",
        "        try:\n",
        "            lang = detect(\" \".join(all_words))\n",
        "        # what!! :( let's see if we can find any text in abstract...\n",
        "        except Exception as e:\n",
        "            \n",
        "            try:\n",
        "                # let's try to label it through the abstract then\n",
        "                lang = detect(df_covid.iloc[ii]['abstract_summary'])\n",
        "            except Exception as e:\n",
        "                lang = \"unknown\"\n",
        "                pass\n",
        "    \n",
        "    # get the language    \n",
        "    languages.append(lang)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeuQgdkPacbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "languages_dict = {}\n",
        "for lang in set(languages):\n",
        "    languages_dict[lang] = languages.count(lang)\n",
        "    \n",
        "print(\"Total: {}\\n\".format(len(languages)))\n",
        "pprint(languages_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plbml0mBbqKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_covid['language'] = languages\n",
        "plt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\n",
        "plt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\n",
        "plt.title(\"Distribution of Languages in Dataset\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSzVRfq-btEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df_covid[df_covid['language'] == 'en'] \n",
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPY3pxsHbyyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the spacy bio parser\n",
        "\n",
        "from IPython.utils import io\n",
        "with io.capture_output() as captured:\n",
        "    !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E67XBNbYb44R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NLP \n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import en_core_sci_lg  # model downloaded in previous step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NmEcj2hc5wC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "punctuations = string.punctuation\n",
        "stopwords = list(STOP_WORDS)\n",
        "stopwords[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8hrY4Xdc8K1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_stop_words = [\n",
        "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
        "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n",
        "    'al.', 'Elsevier', 'PMC', 'CZI', 'www', 'covid-19'\n",
        "]\n",
        "\n",
        "for w in custom_stop_words:\n",
        "    if w not in stopwords:\n",
        "        stopwords.append(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb-iPHsAdATX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parser\n",
        "parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\n",
        "parser.max_length = 7000000\n",
        "\n",
        "def spacy_tokenizer(sentence):\n",
        "    mytokens = parser(sentence)\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
        "    mytokens = \" \".join([i for i in mytokens])\n",
        "    return mytokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwPkqDvsdJM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tqdm.pandas()\n",
        "df[\"processed_text\"] = df[\"body_text\"].progress_apply(spacy_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_2H_pI5dN8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('clean_df.csv')\n",
        "!cp clean_df.csv \"/content/drive/My Drive/Deep Learning Data/Covid-19 Text Mining\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrljFOizTqYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgJ7qQK3T7Wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(df['body_word_count'])\n",
        "df['body_word_count'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLuIFvgUT89P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(df['body_unique_words'])\n",
        "df['body_unique_words'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miZyzKJ8T_Q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}