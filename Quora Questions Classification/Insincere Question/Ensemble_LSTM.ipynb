{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3Up_lkAykpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import time\n",
        "import gc\n",
        "import random\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "\n",
        "tqdm.pandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74-G3_2yzHl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "37f88f98-0427-4f97-fdba-9e2ed489bfe8"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Deep Learning Data/Quora Insincere Question/quora-insincere-questions-classification.zip'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Deep Learning Data/Quora Insincere Question/quora-insincere-questions-classification.zip\n",
            "  inflating: embeddings.zip          \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biHvSIsIzEhL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "c093d00e-6633-4329-f69b-946ab7b28551"
      },
      "source": [
        "!unzip '/content/embeddings.zip'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/embeddings.zip\n",
            "   creating: GoogleNews-vectors-negative300/\n",
            "   creating: glove.840B.300d/\n",
            "   creating: paragram_300_sl999/\n",
            "   creating: wiki-news-300d-1M/\n",
            "  inflating: glove.840B.300d/glove.840B.300d.txt  \n",
            "  inflating: GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin  \n",
            "  inflating: wiki-news-300d-1M/wiki-news-300d-1M.vec  \n",
            "  inflating: paragram_300_sl999/README.txt  \n",
            "  inflating: paragram_300_sl999/paragram_300_sl999.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Oo_UDw00taM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Settings:\n",
        "    \n",
        "# some config values \n",
        "embed_size = 300 # how big is each word vector\n",
        "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 100 # max number of words in a question to use\n",
        "\n",
        "S_DROPOUT = 0.4\n",
        "DROPOUT = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krNoL2Vr1wVb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3c8648e-f4eb-414a-f291-b6d5fee2f0e9"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import gc\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, CuDNNLSTM, concatenate\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, Dropout, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3eSdcAy1ySg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwVzvN4C13yN",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tU98nlQ13J1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "def clean_text(x):\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "    return x\n",
        "\n",
        "def clean_numbers(x):\n",
        "    x = re.sub('[0-9]{5,}', '#####', x)\n",
        "    x = re.sub('[0-9]{4}', '####', x)\n",
        "    x = re.sub('[0-9]{3}', '###', x)\n",
        "    x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x\n",
        "\n",
        "mispell_dict = {\"aren't\" : \"are not\",\n",
        "\"can't\" : \"cannot\",\n",
        "\"couldn't\" : \"could not\",\n",
        "\"didn't\" : \"did not\",\n",
        "\"doesn't\" : \"does not\",\n",
        "\"don't\" : \"do not\",\n",
        "\"hadn't\" : \"had not\",\n",
        "\"hasn't\" : \"has not\",\n",
        "\"haven't\" : \"have not\",\n",
        "\"he'd\" : \"he would\",\n",
        "\"he'll\" : \"he will\",\n",
        "\"he's\" : \"he is\",\n",
        "\"i'd\" : \"I would\",\n",
        "\"i'd\" : \"I had\",\n",
        "\"i'll\" : \"I will\",\n",
        "\"i'm\" : \"I am\",\n",
        "\"isn't\" : \"is not\",\n",
        "\"it's\" : \"it is\",\n",
        "\"it'll\":\"it will\",\n",
        "\"i've\" : \"I have\",\n",
        "\"let's\" : \"let us\",\n",
        "\"mightn't\" : \"might not\",\n",
        "\"mustn't\" : \"must not\",\n",
        "\"shan't\" : \"shall not\",\n",
        "\"she'd\" : \"she would\",\n",
        "\"she'll\" : \"she will\",\n",
        "\"she's\" : \"she is\",\n",
        "\"shouldn't\" : \"should not\",\n",
        "\"that's\" : \"that is\",\n",
        "\"there's\" : \"there is\",\n",
        "\"they'd\" : \"they would\",\n",
        "\"they'll\" : \"they will\",\n",
        "\"they're\" : \"they are\",\n",
        "\"they've\" : \"they have\",\n",
        "\"we'd\" : \"we would\",\n",
        "\"we're\" : \"we are\",\n",
        "\"weren't\" : \"were not\",\n",
        "\"we've\" : \"we have\",\n",
        "\"what'll\" : \"what will\",\n",
        "\"what're\" : \"what are\",\n",
        "\"what's\" : \"what is\",\n",
        "\"what've\" : \"what have\",\n",
        "\"where's\" : \"where is\",\n",
        "\"who'd\" : \"who would\",\n",
        "\"who'll\" : \"who will\",\n",
        "\"who're\" : \"who are\",\n",
        "\"who's\" : \"who is\",\n",
        "\"who've\" : \"who have\",\n",
        "\"won't\" : \"will not\",\n",
        "\"wouldn't\" : \"would not\",\n",
        "\"you'd\" : \"you would\",\n",
        "\"you'll\" : \"you will\",\n",
        "\"you're\" : \"you are\",\n",
        "\"you've\" : \"you have\",\n",
        "\"'re\": \" are\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'll\":\" will\",\n",
        "\"didn't\": \"did not\",\n",
        "\"tryin'\":\"trying\"}\n",
        "\n",
        "def _get_mispell(mispell_dict):\n",
        "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
        "    return mispell_dict, mispell_re\n",
        "\n",
        "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
        "def replace_typical_misspell(text):\n",
        "    def replace(match):\n",
        "        return mispellings[match.group(0)]\n",
        "    return mispellings_re.sub(replace, text)\n",
        "\n",
        "\n",
        "first_word_mispell_dict = {\n",
        "                'whta': 'what', 'howdo': 'how do', 'Whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', \n",
        "                'howmany': 'how many', 'whydo': 'why do', 'doi': 'do i', 'howdoes': 'how does', \"whst\": 'what', \n",
        "                'shoupd': 'should', 'whats': 'what is', \"im\": \"i am\", \"whatis\": \"what is\", \"iam\": \"i am\", \"wat\": \"what\",\n",
        "                \"wht\": \"what\",\"whts\": \"what is\", \"whtwh\": \"what\", \"whtat\": \"what\", \"whtlat\": \"what\", \"dueto to\": \"due to\",\n",
        "                \"dose\": \"does\", \"wha\": \"what\", 'hw': \"how\", \"its\": \"it is\", \"whay\": \"what\", \"ho\": \"how\", \"whart\": \"what\", \n",
        "                \"woe\": \"wow\", \"wt\": \"what\", \"ive\": \"i have\",\"wha\": \"what\", \"wich\": \"which\", \"whic\": \"which\", \"whys\": \"why\", \n",
        "                \"doe\": \"does\", \"wjy\": \"why\", \"wgat\": \"what\", \"hiw\": \"how\",\"howto\": \"how to\", \"lets\": \"let us\", \"haw\": \"how\", \n",
        "                \"witch\": \"which\", \"wy\": \"why\", \"girlfriend\": \"girl friend\", \"hows\": \"how is\",\"whyis\": \"why is\", \"whois\": \"who is\",\n",
        "                \"dont\": \"do not\", \"hat\": \"what\", \"whos\": \"who is\", \"whydoes\": \"why does\", \"whic\": \"which\",\"hy\": \"why\", \"w? hy\": \"why\",\n",
        "                \"ehat\": \"what\", \"whate\": \"what\", \"whai\": \"what\", \"whichis\": \"which is\", \"whi\": \"which\", \"isit\": \"is it\",\"ca\": \"can\", \n",
        "                \"wwhat\": \"what\", \"wil\": \"will\", \"wath\": \"what\", \"plz\": \"please\", \"ww\": \"how\", \"hou\": \"how\", \"whch\": \"which\",\n",
        "                \"ihave\": \"i have\", \"cn\": \"can\", \"doesnt\": \"does not\", \"shoul\": \"should\", \"whatdo\": \"what do\", \"isnt\": \"is not\", \n",
        "                \"whare\": \"what are\",\"whick\": \"which\", \"whatdoes\": \"what does\", \"hwo\": \"how\", \"howdid\": \"how did\", \"why dose\": \"why does\"\n",
        "}\n",
        "def correct_first_word(x):\n",
        "    for key in first_word_mispell_dict.keys():\n",
        "        if x.startswith(key + \" \"):\n",
        "            x = x.replace(key + \" \", first_word_mispell_dict[key] + \" \")\n",
        "            break\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_06hRCN3Eo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 43\n",
        "def seed_torch(seed=43):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPwcQjyP2DhT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "9f954112-002e-4824-f018-f92d24822d7a"
      },
      "source": [
        "def load_and_prec():\n",
        "    train_df = pd.read_csv(\"/content/train.csv\")\n",
        "    test_df = pd.read_csv(\"/content/test.csv\")\n",
        "    print(\"Train shape : \",train_df.shape)\n",
        "    print(\"Test shape : \",test_df.shape)\n",
        "    \n",
        "    # lower\n",
        "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: x.lower())\n",
        "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: x.lower())\n",
        "    \n",
        "    # clean first word\n",
        "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: correct_first_word(x))\n",
        "    \n",
        "    # Clean the text\n",
        "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
        "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
        "    \n",
        "    # Clean numbers\n",
        "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
        "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
        "    \n",
        "    # Clean speelings\n",
        "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
        "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
        "    \n",
        "    ## fill up the missing values\n",
        "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
        "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
        "\n",
        "    ## Tokenize the sentences\n",
        "    tokenizer = Tokenizer(num_words=max_features, oov_token='OOV', filters='')\n",
        "    tokenizer.fit_on_texts(list(train_X))\n",
        "    train_X = tokenizer.texts_to_sequences(train_X)\n",
        "    test_X = tokenizer.texts_to_sequences(test_X)\n",
        "    print(\"tokenization finished, word count: %d\" % len(tokenizer.word_index))\n",
        "\n",
        "    ## Pad the sentences \n",
        "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
        "\n",
        "    ## Get the target values\n",
        "    train_y = train_df['target'].values\n",
        "    \n",
        "    #shuffling the data\n",
        "    np.random.seed(SEED)\n",
        "    trn_idx = np.random.permutation(len(train_X))\n",
        "\n",
        "    train_X = train_X[trn_idx]\n",
        "    train_y = train_y[trn_idx]\n",
        "    \n",
        "    return train_X, test_X, train_y, tokenizer.word_index\n",
        "train_X, test_X, train_y, word_index = load_and_prec()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 54209/1306122 [00:00<00:02, 542089.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (1306122, 3)\n",
            "Test shape :  (375806, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1306122/1306122 [00:01<00:00, 861811.62it/s]\n",
            "100%|██████████| 375806/375806 [00:00<00:00, 790970.59it/s]\n",
            "100%|██████████| 375806/375806 [00:05<00:00, 68675.40it/s]\n",
            "100%|██████████| 1306122/1306122 [00:39<00:00, 33147.39it/s]\n",
            "100%|██████████| 375806/375806 [00:11<00:00, 33347.58it/s]\n",
            "100%|██████████| 1306122/1306122 [00:14<00:00, 93069.59it/s]\n",
            "100%|██████████| 375806/375806 [00:04<00:00, 93015.83it/s]\n",
            "100%|██████████| 1306122/1306122 [00:12<00:00, 108810.68it/s]\n",
            "100%|██████████| 375806/375806 [00:03<00:00, 108644.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tokenization finished, word count: 188179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pArKaG_j2aK2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea987f66-9935-4129-c547-7d4419b8240d"
      },
      "source": [
        "test_X.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(375806, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1LH5IXQ42xA",
        "colab_type": "text"
      },
      "source": [
        "# **Loading Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROfb9l7Y4We-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_glove(word_index):\n",
        "    EMBEDDING_FILE = '/content/glove.840B.300d/glove.840B.300d.txt'    \n",
        "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
        "    nb_words = min(max_features, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "\n",
        "    with open(EMBEDDING_FILE, 'r', encoding=\"utf8\") as f:\n",
        "        for line in tqdm(f):\n",
        "            word, vec = line.split(' ', 1)\n",
        "            if word not in word_index:\n",
        "                continue\n",
        "            i = word_index[word]\n",
        "            if i >= nb_words:\n",
        "                continue\n",
        "            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
        "            if len(embedding_vector) == 300:\n",
        "                embedding_matrix[i] = embedding_vector                \n",
        "            \n",
        "    return embedding_matrix\n",
        "    \n",
        "def load_fasttext(word_index):    \n",
        "    EMBEDDING_FILE = '/content/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
        "    emb_mean, emb_std = -0.0033469985, 0.109855495\n",
        "    nb_words = min(max_features, len(word_index) + 1)\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    with open(EMBEDDING_FILE, 'r') as f:\n",
        "        for line in tqdm(f):\n",
        "            if len(line) <= 100:\n",
        "                continue\n",
        "            word, vec = line.split(' ', 1)\n",
        "            if word not in word_index:\n",
        "                continue\n",
        "            i = word_index[word]\n",
        "            if i >= nb_words:\n",
        "                continue\n",
        "            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
        "            if len(embedding_vector) == 300:\n",
        "                embedding_matrix[i] = embedding_vector   \n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def load_para(word_index):\n",
        "    EMBEDDING_FILE = '/content/paragram_300_sl999/paragram_300_sl999.txt'\n",
        "    emb_mean, emb_std = -0.0053247833, 0.49346462\n",
        "    nb_words = min(max_features, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    with open(EMBEDDING_FILE, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
        "        for line in tqdm(f):\n",
        "            if len(line) <= 100:\n",
        "                continue\n",
        "            word, vec = line.split(' ', 1)\n",
        "            if word not in word_index:\n",
        "                continue\n",
        "            i = word_index[word]\n",
        "            if i >= nb_words:\n",
        "                continue\n",
        "            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
        "            if len(embedding_vector) == 300:\n",
        "                embedding_matrix[i] = embedding_vector \n",
        "    \n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHJIrK6R4v7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4c35995b-4e61-4f8b-f3a4-945f70e13b31"
      },
      "source": [
        "embedding_matrix_1 = load_glove(word_index)\n",
        "embedding_matrix_2 = load_fasttext(word_index)\n",
        "embedding_matrix_3 = load_para(word_index)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2196017it [01:43, 21134.06it/s]\n",
            "999995it [00:40, 24869.01it/s]\n",
            "1703756it [01:20, 21162.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlZzf4uU41AE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix123 = np.concatenate([embedding_matrix_1, embedding_matrix_2, embedding_matrix_3], axis = 1)\n",
        "embedding_matrix12 = np.concatenate([embedding_matrix_1, embedding_matrix_2], axis = 1)\n",
        "embedding_matrix13 = np.concatenate([embedding_matrix_1, embedding_matrix_3], axis = 1)\n",
        "embedding_matrix23 = np.concatenate([embedding_matrix_2, embedding_matrix_3], axis = 1)\n",
        "embedding_matrixs = [embedding_matrix12, embedding_matrix13, embedding_matrix23, embedding_matrix13]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XOdkOLi9znA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###LSTM Model\n",
        "inp = Input(shape=(maxlen,))\n",
        "x = Embedding(max_features, embed_size * 3, weights=[embedding_matrix123])(inp)\n",
        "x = SpatialDropout1D(S_DROPOUT)(x)\n",
        "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "avg_pool = GlobalAveragePooling1D()(x)\n",
        "max_pool = GlobalMaxPooling1D()(x)\n",
        "conc = concatenate([avg_pool, max_pool])\n",
        "x = Dense(16, activation=\"relu\")(conc)\n",
        "x = Dropout(DROPOUT)(x)\n",
        "x = Dense(1, activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ImYkWfB_a55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size = 0.15, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnWiHqRc-iia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))\n",
        "## take too long to finish training , so i will not finish the training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE8ipeL9_7p1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb4f9ccd-21b4-49b9-9b02-623f64103873"
      },
      "source": [
        "pred_val_lstm_y = model.predict([val_X], batch_size=1024, verbose=1)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "195919/195919 [==============================] - 329s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auxQTTdHLPo8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc0129ef-7ee3-451c-c138-e70678870dea"
      },
      "source": [
        "pred_test_lstm_y = model.predict([test_X], batch_size=1024, verbose=1)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "375806/375806 [==============================] - 638s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpYgZPnxMkvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.kaggle.com/yekenot/2dcnn-textclassifier\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "\n",
        "filter_sizes = [1,2,3,5]\n",
        "num_filters = 36\n",
        "\n",
        "inp = Input(shape=(maxlen,))\n",
        "x = Embedding(max_features, embed_size * 3, weights=[embedding_matrix123])(inp)\n",
        "x = SpatialDropout1D(S_DROPOUT)(x)\n",
        "x = Reshape((maxlen, embed_size * 3, 1))(x)\n",
        "\n",
        "maxpool_pool = []\n",
        "for i in range(len(filter_sizes)):\n",
        "    conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size * 3),\n",
        "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
        "    maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n",
        "\n",
        "z = Concatenate(axis=1)(maxpool_pool)   \n",
        "z = Flatten()(z)\n",
        "z = Dropout(DROPOUT)(z)\n",
        "\n",
        "outp = Dense(1, activation=\"sigmoid\")(z)\n",
        "\n",
        "model = Model(inputs=inp, outputs=outp)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPD2HiE5Pv64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))\n",
        "#again, can't wait for it to finish training, i stop when accuracy reach 0.94\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLsui6sKQFoV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e596652-dd28-4272-b527-bf9bdc6c39aa"
      },
      "source": [
        "pred_val_cnn_y = model.predict([val_X], batch_size=1024, verbose=1)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "195919/195919 [==============================] - 172s 877us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAXs5A-5URvI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "893172bf-ec80-47b7-eb84-12886ead1f72"
      },
      "source": [
        "pred_test_cnn_y = model.predict([test_X], batch_size=1024, verbose=1)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "375806/375806 [==============================] - 326s 869us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD9WG_81UTIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc; gc.collect()\n",
        "time.sleep(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBTtMOCQXEAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "outputId": "1113864c-ddbf-4e52-d8a3-9e7651ce3642"
      },
      "source": [
        "pred_val_y = 0.6 * pred_val_lstm_y + 0.4 * pred_val_cnn_y  # two random numbers :)\n",
        "pred_test_y = 0.6 * pred_test_lstm_y + 0.4 * pred_test_cnn_y \n",
        "\n",
        "thresholds = []\n",
        "for thresh in np.arange(0.1, 0.501, 0.01):\n",
        "    thresh = np.round(thresh, 2)\n",
        "    res = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
        "    thresholds.append([thresh, res])\n",
        "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
        "    \n",
        "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
        "best_thresh = thresholds[0][0]\n",
        "print(\"Best threshold: \", best_thresh)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score at threshold 0.1 is 0.5860140257232901\n",
            "F1 score at threshold 0.11 is 0.5947550717466601\n",
            "F1 score at threshold 0.12 is 0.6031528444139822\n",
            "F1 score at threshold 0.13 is 0.6082408392884294\n",
            "F1 score at threshold 0.14 is 0.614945559450321\n",
            "F1 score at threshold 0.15 is 0.6190355811160968\n",
            "F1 score at threshold 0.16 is 0.6224607868346619\n",
            "F1 score at threshold 0.17 is 0.6257668711656443\n",
            "F1 score at threshold 0.18 is 0.6290964574764494\n",
            "F1 score at threshold 0.19 is 0.6312207284612015\n",
            "F1 score at threshold 0.2 is 0.6334130509053638\n",
            "F1 score at threshold 0.21 is 0.6362092273562342\n",
            "F1 score at threshold 0.22 is 0.6386176863103181\n",
            "F1 score at threshold 0.23 is 0.6394194856472095\n",
            "F1 score at threshold 0.24 is 0.6421932415880107\n",
            "F1 score at threshold 0.25 is 0.644051739843323\n",
            "F1 score at threshold 0.26 is 0.6451137243336897\n",
            "F1 score at threshold 0.27 is 0.6452576549663929\n",
            "F1 score at threshold 0.28 is 0.645702623025172\n",
            "F1 score at threshold 0.29 is 0.6472120098039216\n",
            "F1 score at threshold 0.3 is 0.6471111799078982\n",
            "F1 score at threshold 0.31 is 0.6469021696561447\n",
            "F1 score at threshold 0.32 is 0.6465271170313986\n",
            "F1 score at threshold 0.33 is 0.6457273346420416\n",
            "F1 score at threshold 0.34 is 0.644735777292223\n",
            "F1 score at threshold 0.35 is 0.6431318007035917\n",
            "F1 score at threshold 0.36 is 0.6414252056379944\n",
            "F1 score at threshold 0.37 is 0.6398662766402006\n",
            "F1 score at threshold 0.38 is 0.6370245139475909\n",
            "F1 score at threshold 0.39 is 0.6333618477331052\n",
            "F1 score at threshold 0.4 is 0.6295783184691315\n",
            "F1 score at threshold 0.41 is 0.627366760168303\n",
            "F1 score at threshold 0.42 is 0.6241738744732757\n",
            "F1 score at threshold 0.43 is 0.61910759265923\n",
            "F1 score at threshold 0.44 is 0.6139128057947246\n",
            "F1 score at threshold 0.45 is 0.6096379986165552\n",
            "F1 score at threshold 0.46 is 0.6028985507246376\n",
            "F1 score at threshold 0.47 is 0.5965911788444191\n",
            "F1 score at threshold 0.48 is 0.5906292091591303\n",
            "F1 score at threshold 0.49 is 0.5823044263335123\n",
            "F1 score at threshold 0.5 is 0.5734529059109151\n",
            "Best threshold:  0.29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBJecrSPXSeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}