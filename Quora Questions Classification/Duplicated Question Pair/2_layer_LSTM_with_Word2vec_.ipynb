{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-layer LSTM with Word2vec .ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oZ2AP7sEVN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "519FSbb2KUU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/My Drive/Deep Learning Data/Quora Duplicated Question/train.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDAQkXcKL-lj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "002abde2-2a4a-4be2-d28f-8c6a534cab89"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Deep Learning Data/Word2Vec Embeddings/glove.840B.300d.zip'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Deep Learning Data/Word2Vec Embeddings/glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi7YaeZNLe8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read pretrained- embedding. I use Glove for this problem\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6s2RBaSNO4g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e06c369e-7f25-4c0d-bc4d-bb2a353ff709"
      },
      "source": [
        "glove_file = datapath('/content/glove.840B.300d.txt')\n",
        "word2vec_glove_file = get_tmpfile(\"glove.840B.300d.txt\")\n",
        "glove2word2vec(glove_file, word2vec_glove_file)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2196017, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLfiM7dPPHtB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ae5968f9-ac5b-425e-cfcb-ae0444473b0d"
      },
      "source": [
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67ccXyB4QQx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "    \n",
        "    # Return a list of words\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTOz6havUQbU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5f8463b-6d17-4356-8a62-c5702a2b6ace"
      },
      "source": [
        "texts_1 = [] \n",
        "texts_2 = []\n",
        "labels = []\n",
        "with codecs.open('/content/drive/My Drive/Deep Learning Data/Quora Duplicated Question/train.csv', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter=',')\n",
        "    header = next(reader)\n",
        "    for values in reader:\n",
        "        texts_1.append(text_to_wordlist(values[3]))\n",
        "        texts_2.append(text_to_wordlist(values[4]))\n",
        "        labels.append(int(values[5]))\n",
        "print('Found %s texts in train.csv' % len(texts_1))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 404290 texts in train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4V93fIUSz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 30\n",
        "MAX_NB_WORDS = 200000\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QYbkcxbZ5lM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "515c618d-4058-4be3-a3d3-d46a8b50a2f8"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts_1 + texts_2 )\n",
        "\n",
        "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
        "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens' % len(word_index))\n",
        "\n",
        "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = np.array(labels)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 85518 unique tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euIm4pGYaFNi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a1415bc7-f641-4c11-e824-e22083ac109a"
      },
      "source": [
        "\n",
        "print('Shape of data tensor:', data_1.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (404290, 30)\n",
            "Shape of label tensor: (404290,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFxu7-3tafHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3b6b7fee-aa60-44fc-db0d-8f8ba6ec947d"
      },
      "source": [
        "\n",
        "print('Preparing embedding matrix')\n",
        "\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if word in model.vocab:\n",
        "        embedding_matrix[i] = model.word_vec(word)\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix\n",
            "Null word embeddings: 18731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdPDonFTawDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "## sample train/validation data\n",
        "########################################\n",
        "np.random.seed(1234)\n",
        "perm = np.random.permutation(len(data_1))\n",
        "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
        "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
        "\n",
        "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
        "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
        "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
        "\n",
        "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
        "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
        "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkh41wMLa9i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
        "\n",
        "weight_val = np.ones(len(labels_val))\n",
        "if re_weight:\n",
        "    weight_val *= 0.472001959\n",
        "    weight_val[labels_val==0] = 1.309028344"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2WLMlM7bYOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_lstm = np.random.randint(175, 275)\n",
        "num_dense = np.random.randint(100, 150)\n",
        "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
        "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9zOGnsnUlRZa",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 30\n",
        "MAX_NB_WORDS = 200000\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EOjpAIVk_3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "embedding_layer = Embedding(nb_words,\n",
        "        EMBEDDING_DIM,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=MAX_SEQUENCE_LENGTH,\n",
        "        trainable=False)\n",
        "lstm_layer_1= LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm, return_sequences= True)\n",
        "lstm_layer_2 = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
        "\n",
        "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
        "x1 = lstm_layer_1(embedded_sequences_1)\n",
        "x2 = lstm_layer_2(x1)\n",
        "\n",
        "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
        "y1 = lstm_layer_1(embedded_sequences_2)\n",
        "y2 = lstm_layer_2(y1)\n",
        "\n",
        "merged = concatenate([x2, y2])\n",
        "merged = Dropout(rate_drop_dense)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "\n",
        "merged = Dense(num_dense, activation=act)(merged)\n",
        "merged = Dropout(rate_drop_dense)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "\n",
        "preds = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "########################################\n",
        "## add class weight\n",
        "########################################\n",
        "if re_weight:\n",
        "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
        "else:\n",
        "    class_weight = None\n",
        "\n",
        "########################################\n",
        "## train the model\n",
        "########################################\n",
        "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
        "        outputs=preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNfwEVEcbBmS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "21fe8d2f-8a5b-4974-8970-016ca47fbcd6"
      },
      "source": [
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "        optimizer='nadam',\n",
        "        metrics=['acc'])\n",
        "model.summary()\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           (None, 30)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_27 (InputLayer)           (None, 30)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_16 (Embedding)        (None, 30, 300)      25655700    input_26[0][0]                   \n",
            "                                                                 input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_17 (LSTM)                  (None, 30, 193)      381368      embedding_16[0][0]               \n",
            "                                                                 embedding_16[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_18 (LSTM)                  (None, 193)          298764      lstm_17[0][0]                    \n",
            "                                                                 lstm_17[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 386)          0           lstm_18[0][0]                    \n",
            "                                                                 lstm_18[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 386)          0           concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 386)          1544        dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 149)          57663       batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 149)          0           dense_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 149)          596         dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 1)            150         batch_normalization_21[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 26,395,785\n",
            "Trainable params: 739,015\n",
            "Non-trainable params: 25,656,770\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6byDpcbhbTdv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "f1076c66-d1f8-4726-c6d8-d3e51e230513"
      },
      "source": [
        "\n",
        "\n",
        "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
        "bst_model_path = STAMP + '.h5'\n",
        "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
        "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
        "        epochs=200, batch_size=2048, shuffle=True, \\\n",
        "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "model.load_weights(bst_model_path)\n",
        "bst_val_score = min(hist.history['val_loss'])\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 727722 samples, validate on 80858 samples\n",
            "Epoch 1/200\n",
            "727722/727722 [==============================] - 118s 162us/step - loss: 0.4023 - acc: 0.6872 - val_loss: 0.3558 - val_acc: 0.6918\n",
            "Epoch 2/200\n",
            "727722/727722 [==============================] - 115s 158us/step - loss: 0.3352 - acc: 0.7286 - val_loss: 0.3205 - val_acc: 0.7450\n",
            "Epoch 3/200\n",
            "727722/727722 [==============================] - 116s 159us/step - loss: 0.3114 - acc: 0.7511 - val_loss: 0.2977 - val_acc: 0.7741\n",
            "Epoch 4/200\n",
            "727722/727722 [==============================] - 116s 159us/step - loss: 0.2938 - acc: 0.7683 - val_loss: 0.2889 - val_acc: 0.7927\n",
            "Epoch 5/200\n",
            "727722/727722 [==============================] - 115s 158us/step - loss: 0.2795 - acc: 0.7828 - val_loss: 0.2822 - val_acc: 0.8103\n",
            "Epoch 6/200\n",
            "727722/727722 [==============================] - 116s 159us/step - loss: 0.2675 - acc: 0.7953 - val_loss: 0.2768 - val_acc: 0.8183\n",
            "Epoch 7/200\n",
            "727722/727722 [==============================] - 116s 159us/step - loss: 0.2573 - acc: 0.8051 - val_loss: 0.2751 - val_acc: 0.8223\n",
            "Epoch 8/200\n",
            "727722/727722 [==============================] - 115s 159us/step - loss: 0.2479 - acc: 0.8147 - val_loss: 0.2701 - val_acc: 0.8245\n",
            "Epoch 9/200\n",
            "727722/727722 [==============================] - 115s 159us/step - loss: 0.2401 - acc: 0.8222 - val_loss: 0.2742 - val_acc: 0.8320\n",
            "Epoch 10/200\n",
            "727722/727722 [==============================] - 115s 158us/step - loss: 0.2336 - acc: 0.8288 - val_loss: 0.2713 - val_acc: 0.8283\n",
            "Epoch 11/200\n",
            "727722/727722 [==============================] - 116s 159us/step - loss: 0.2275 - acc: 0.8338 - val_loss: 0.2693 - val_acc: 0.8357\n",
            "Epoch 12/200\n",
            "727722/727722 [==============================] - 116s 160us/step - loss: 0.2216 - acc: 0.8402 - val_loss: 0.2678 - val_acc: 0.8319\n",
            "Epoch 13/200\n",
            "727722/727722 [==============================] - 116s 159us/step - loss: 0.2173 - acc: 0.8435 - val_loss: 0.2759 - val_acc: 0.8386\n",
            "Epoch 14/200\n",
            "727722/727722 [==============================] - 115s 158us/step - loss: 0.2128 - acc: 0.8475 - val_loss: 0.2640 - val_acc: 0.8351\n",
            "Epoch 15/200\n",
            "727722/727722 [==============================] - 115s 159us/step - loss: 0.2084 - acc: 0.8513 - val_loss: 0.2766 - val_acc: 0.8414\n",
            "Epoch 16/200\n",
            "727722/727722 [==============================] - 115s 159us/step - loss: 0.2057 - acc: 0.8539 - val_loss: 0.2753 - val_acc: 0.8384\n",
            "Epoch 17/200\n",
            "727722/727722 [==============================] - 115s 158us/step - loss: 0.2024 - acc: 0.8572 - val_loss: 0.2692 - val_acc: 0.8389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maS8elvpgA-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "1d79c012-dd81-4695-b3a8-dec12e2a020a"
      },
      "source": [
        "model_save_name = '2_layer_LSTM.pt'\n",
        "path = F\"/content/drive/My Drive/Deep Learning Data/Quora Duplicated Question/{model_save_name}\" \n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-eeb0e02a31ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2_layer_LSTM.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mF\"/content/drive/My Drive/Deep Learning Data/Quora Duplicated Question/{model_save_name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khiX9fCG29y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}