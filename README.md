# Natural-Language-Processing
This repository is dedicated for my Natural Language Processing Progress

1. Covid-19 Open Research Challenges: https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge
    - In contribution for research community to fight against Covid-19 pandemic
    
    - Goals : 
    
          - EDA for 30000+ research articles (archieved)
          
          - Contextual Document Embedding with Tf-idf and BERT contextual title embedding (on-going)
          
          - Unsupervised topic clustering and Visualization with LDA, PCA, K-Means, t-SNE (on-going)
          
          - Sematic Search with sentence-bert embeddings (archieved)
          
          
2. Multilingual Toxic Comment Classification: 3rd part of Toxic Comment Classification. 
      (https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification)
      
      - Goal: Build a model that trained to detect toxic comments in English and generalizing for validation and test sets in other languages.
      
      - My attempts :
      
           - Notebook 1: EDA for toxic comments and possible solutions for heavily imbalanced dataset (on-going)
                
           - Notebook 2: Adapted and Fine-tuned multilingual language models (mBERT, XLM-RoBERTa) for classification problems (on-going)
                
           - Notebook 3: Translate selective comments on both training and validation set to enrich datasource and generalize for test set 
         
  3. Transfer Learning and Knowledge Distillation
  
       - Goal: Reducing the size of pretrained knowledge while retaining performance
       
       - Techniques: 
            
            - Transfer Learning: Adapt and fine-tune pretrained models for domain-specific text
            
            - Knowledge Distillation: Distill knowledge of pretrained big models (teachers) to smaller networks (students)
            
            - Model, Weight Pruning: Possible layers, nodes, and weight dropout during inference time
