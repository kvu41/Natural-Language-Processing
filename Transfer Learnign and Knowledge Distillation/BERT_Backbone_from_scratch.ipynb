{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJrqe_MzrkFr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "outputId": "50b55210-986e-48ba-a632-edbe79fc2c27"
      },
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-ignite ipdb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hCollecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/55/41e8a995876fd2ade29bdba0c3efefa38e7d605cb353c70f3173c04928b5/pytorch_ignite-0.3.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 7.8MB/s \n",
            "\u001b[?25hCollecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/bb/a3e1a441719ebd75c6dac8170d3ddba884b7ee8a5c0f9aefa7297386627a/ipdb-0.13.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.40)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (46.1.3)\n",
            "Requirement already satisfied: ipython>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.15.40)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (2.1.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.3.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.4.2)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (1.12.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (0.1.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0->ipdb) (0.6.0)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.2-cp36-none-any.whl size=10522 sha256=20649f49704e805b5b25bdd11a4a60c9efd8d9716d031c44a8127e2f1f9bb0b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/c2/15/793365e3c9318c46ba914263740d90f1fe67f544b979141ce4\n",
            "Successfully built ipdb\n",
            "Installing collected packages: pytorch-pretrained-bert, pytorch-ignite, ipdb\n",
            "Successfully installed ipdb-0.13.2 pytorch-ignite-0.3.0 pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZopP9Juhs_nj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07c82802-085c-43df-ca7b-22c5dda3abea"
      },
      "source": [
        "\"\"\" Creating  GPT-2 architech (with the use of ReLu activation instead of geLu from original paper.\n",
        "You can find the original paper @ Languages Models are Unsupervised Multitask Learners (Radford et al., 2018) \"\"\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Creating  GPT-2 architech (with the use of ReLu activation instead of geLu from original paper.\\nYou can find the original paper @ Languages Models are Unsupervised Multitask Learners (Radford et al., 2018) '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go9Spwnjwhwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, num_embeddings, num_max_positions, num_heads, num_layers, dropout, causal):\n",
        "      #Args :\n",
        "              #-embed_dim: embedding dimensions\n",
        "              #-hidden_dim: hidden dimensions of feed_forward \n",
        "              #-num_embeddings: vocabulary size\n",
        "              #-num_max_positions: max_length of a sequence\n",
        "              #-num_heads: number of heads of self.attention layer\n",
        "              #-num_layers: number of transformer blocks\n",
        "              #-dropout: dropout probability\n",
        "              #-causal: None/True. Whether or not applying causal LMs with attention masks \n",
        "        super().__init__()\n",
        "        self.causal = causal\n",
        "        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n",
        "        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n",
        "        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.attentions.append(nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))\n",
        "            self.feed_forwards.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n",
        "                                                    nn.ReLU(),\n",
        "                                                    nn.Linear(hidden_dim, embed_dim)))\n",
        "            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
        "            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
        "\n",
        "    def forward(self, x, padding_mask=None):\n",
        "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
        "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n",
        "        h = self.tokens_embeddings(x)\n",
        "        h = h + self.position_embeddings(positions).expand_as(h)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        attn_mask = None\n",
        "        if self.causal:\n",
        "            attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)\n",
        "            attn_mask = torch.triu(attn_mask, diagonal=1)\n",
        "\n",
        "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,\n",
        "                                                                       self.layer_norms_2, self.feed_forwards):\n",
        "            h = layer_norm_1(h)\n",
        "            x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)\n",
        "            x = self.dropout(x)\n",
        "            h = x + h\n",
        "\n",
        "            h = layer_norm_2(h)\n",
        "            x = feed_forward(h)\n",
        "            x = self.dropout(x)\n",
        "            h = x + h\n",
        "            print(h)\n",
        "        return h\n",
        "      \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq_jYSh5Id7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3376aee9-354a-4f01-ad00-cc845f7d9a47"
      },
      "source": [
        "\"\"\" Adding Language Modelling layer on top of Transformer\n",
        "\"\"\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Adding Language Modelling layer on top of Transformer\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv-3L3JdEJyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerWithLMHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        \"\"\" Transformer with a language modeling head on top (tied weights) \"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = Transformer(config.embed_dim, config.hidden_dim, config.num_embeddings,\n",
        "                                       config.num_max_positions, config.num_heads, config.num_layers,\n",
        "                                       config.dropout, causal=not config.mlm)\n",
        "\n",
        "        self.lm_head = nn.Linear(config.embed_dim, config.num_embeddings, bias=False)\n",
        "        self.apply(self.init_weights)\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        self.lm_head.weight = self.transformer.tokens_embeddings.weight\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        \"\"\" initialize weights - nn.MultiheadAttention is already initalized by PyTorch (xavier) \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, labels=None, padding_mask=None):\n",
        "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
        "        hidden_states = self.transformer(x, padding_mask)\n",
        "        logits = self.lm_head(hidden_states)\n",
        "\n",
        "        if labels is not None:\n",
        "            shift_logits = logits[:-1] if self.transformer.causal else logits\n",
        "            shift_labels = labels[1:] if self.transformer.causal else labels\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            return logits, loss\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muU4QtWwIccN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}